{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet Lab\n",
    "![LeNet Architecture](assets/images/lenet.png)\n",
    "Source: Yan LeCun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Train Accuracy = 98.3% and Validation Accuracy = 94.8% for V2\n",
    "\n",
    "V3 adding -15 to 15, inc 5, 6 rotations. leet's see if validation accuracy increases. Before rotation was -10 to 10, inc 5, 4 rotations in V2. Updated learning rate = 0.001 and sigma = 0.1. Updated EPOCHS = 10 and BATCH_SIZE = 64.\n",
    "\n",
    "Train Accuracy = 0.997\n",
    "\n",
    "Validation Accuracy = 0.976\n",
    "\n",
    "Test Accuracy = 0.953"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the **German Traffic Sign Dataset**, which comes pre-loaded with TensorFlow.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "34799\n",
      "[41 41 41 ... 25 25 25]\n"
     ]
    }
   ],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = 'assets/data/train.p'\n",
    "validation_file='assets/data/valid.p'\n",
    "testing_file= 'assets/data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "print(type(X_train))\n",
    "print(len(y_train))\n",
    "print(y_train)\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "\n",
    "View a sample from the dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image)\n",
    "print(X_train[0].dtype)\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def display_result(orig_img, res_img, title = 'Image', show = 1):\n",
    "    if show == 1:\n",
    "        f, (ax1, ax2) = plt.subplots(1,2)\n",
    "#         ax1.set_title('original')\n",
    "        ax1.imshow(orig_img)\n",
    "#         ax2.set_title('result')\n",
    "        ax2.imshow(res_img)\n",
    "#         f.suptitle(title)\n",
    "    \n",
    "def adjust_img_lightness(img, alpha = 1, beta = 20):\n",
    "    \"\"\"Increase brightness of image when beta > 0\n",
    "       Decrease brightness of image when beta < 0\n",
    "       alpha is contrast control, beta is brightness\"\"\"\n",
    "    new_image = np.zeros(img.shape, img.dtype)\n",
    "\n",
    "    # Change brightness of image by accessing the pixels\n",
    "    new_image = cv2.convertScaleAbs(img, alpha = alpha, beta = beta)\n",
    "    return new_image\n",
    "\n",
    "image = X_train[285].squeeze()\n",
    "bright_img = adjust_img_lightness(image, beta = 20)\n",
    "dark_img = adjust_img_lightness(image, beta = -20)\n",
    "\n",
    "display_result(image, bright_img, 'Update Img Brightness')\n",
    "\n",
    "# plt.figure(figsize=(1,1))\n",
    "# plt.imshow(s_p_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization on Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "# normalize images to ensure pixel values are between -0.5, 0.5\n",
    "def normalize_images(images):\n",
    "    norm_images = []\n",
    "    for image in images:\n",
    "        norm_img = (image / 255.0) - 0.5\n",
    "        norm_images.append(norm_img)\n",
    "    return norm_images\n",
    "        \n",
    "# data preprocessing done by tensorflow model\n",
    "# normalization applied\n",
    "X_train = normalize_images(X_train)\n",
    "print(X_train[0].dtype)\n",
    "X_valid = normalize_images(X_valid)\n",
    "X_test = normalize_images(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply **in-place data augmentation**, so our network when trained sees new variations of our data at each and every epoch. So far I augment the dataset using all flip operations on each image and rotation operations on each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n",
      "<class 'list'>\n",
      "208794\n",
      "<class 'list'>\n",
      "208794\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import cv2\n",
    "\n",
    "def adjust_img_lightness(img, alpha = 1, beta = 20):\n",
    "    \"\"\"Increase brightness of image when beta > 0\n",
    "       Decrease brightness of image when beta < 0\n",
    "       alpha is contrast control, beta is brightness\"\"\"\n",
    "    new_image = np.zeros(img.shape, img.dtype)\n",
    "\n",
    "    # Change brightness of image by accessing the pixels\n",
    "    new_image = cv2.convertScaleAbs(img, alpha = alpha, beta = beta)\n",
    "    return new_image\n",
    "\n",
    "# Add Salt and Pepper noise to image\n",
    "# func: display_result(),to_std_uint8(),to_std_float(),add_salt_pepper_noise()\n",
    "def display_result(orig_img, res_img, title = 'Image', show = 1):\n",
    "    if show == 1:\n",
    "        f, (ax1, ax2) = plt.subplots(1,2)\n",
    "#         ax1.set_title('original')\n",
    "        ax1.imshow(orig_img)\n",
    "#         ax2.set_title('result')\n",
    "        ax2.imshow(res_img)\n",
    "#         f.suptitle(title)\n",
    "\n",
    "def to_std_uint8(img):\n",
    "    # Handles the conversion from float16 to uint8\n",
    "    img = cv2.convertScaleAbs(img, alpha = (255/1))\n",
    "    \n",
    "    return img\n",
    "\n",
    "def to_std_float(img):\n",
    "    # Converts img from 0 to 1 float to avoid wrapping that occurs with uint8\n",
    "    img.astype(np.float16, copy = False)\n",
    "    img = np.multiply(img, (1/255))\n",
    "\n",
    "    return img\n",
    "    \n",
    "# when pad is low, there is a lot more noise dots\n",
    "# when pad is high, there is very little noise dots\n",
    "def add_salt_pepper_noise(img, pad = 101, show = 1):\n",
    "    # copy original img\n",
    "    orig_img = img\n",
    "    # Converts img from 0 to 1 float to avoid wrapping that occurs with uint8\n",
    "    img = to_std_float(img)\n",
    "    \n",
    "    # Generate a set of random ints used to create s&p noise\n",
    "    noise = np.random.randint(pad, size = (img.shape[0], img.shape[1], 1))\n",
    "\n",
    "    # Convert high and low bounds of pad in noise to salt and pepper noise then\n",
    "    # add it to image. 1 is subtracted from pad to match bounds behavior of\n",
    "    # np.random.randint\n",
    "    img = np.where(noise == 0, 0, img)\n",
    "    img = np.where(noise == (pad - 1), 1, img)\n",
    "    \n",
    "    # Handles the conversion from float16 back to uint8\n",
    "    # we now have an image with s&p noise\n",
    "    res_img = to_std_uint8(img)\n",
    "    \n",
    "    #display_result(orig_img, res_img, 'Salt & Pepper Noise', show)\n",
    "    \n",
    "    return res_img\n",
    "\n",
    "# The network has to recognize the object present in any orientation\n",
    "def augment_data(images, labels):\n",
    "    # initialize labels\n",
    "    image_labels = []\n",
    "    # initialize augmented data\n",
    "    augmented_images = []\n",
    "    # initialize flip image parameters\n",
    "    flipped_image = None\n",
    "    # initialize rotate image parameters\n",
    "    rotated_image = None\n",
    "    dimension = None\n",
    "    (h, w) = images[0].shape[:2]\n",
    "    center = (w/2, h/2)\n",
    "    rot_angles = [-15, -10, -5, 5, 10, 15]\n",
    "    scale = 1.0\n",
    "    # initialize image translation\n",
    "    # translate image at four sides retaining 87.5% of base img\n",
    "    shift = [(8,8), (-8,8), (-8,-8), (8,-8)]    \n",
    "    \n",
    "    for image, label in zip(images, labels):\n",
    "#         augmented_images.append(image)\n",
    "#         image_labels.append(label)\n",
    "        # 1 Add Salt and Pepper Noise to original image\n",
    "#         s_p_img = add_salt_pepper_noise(image, pad = 101)\n",
    "#         augmented_images.append(s_p_img)\n",
    "#         image_labels.append(label)\n",
    "        \n",
    "        #2 Change the Brightness of the image\n",
    "#         bright_img = adjust_img_lightness(image, beta = 20)\n",
    "#         dark_img = adjust_img_lightness(image, beta = -20)\n",
    "#         # Add brighter images to training dataset\n",
    "#         augmented_images.append(bright_img)\n",
    "#         image_labels.append(label)\n",
    "        # Add darker images to training dataset\n",
    "#         augmented_images.append(dark_img)\n",
    "#         image_labels.append(label)\n",
    "        \n",
    "        # 3: horizontal flip\n",
    "#         flipped_image = cv2.flip(image, 1)\n",
    "#         augmented_images.append(flipped_image)\n",
    "#         image_labels.append(label)\n",
    "        \n",
    "        #3 flip image: loop -1 to 1 to save all flip ops\n",
    "#         for flip_op in range(-1, 2):   \n",
    "#             # flip image randomly: horizontal or vertical\n",
    "#             # 0: horizontal flip, 1: vertical flip, -1: Both flip\n",
    "#             flipped_image = cv2.flip(image, flip_op)\n",
    "#             augmented_images.append(flipped_image)\n",
    "#             image_labels.append(label)\n",
    "            \n",
    "            #3.1 Add Salt and Pepper Noise to flipped image\n",
    "#             flip_s_p_img = add_salt_pepper_noise(flipped_image, pad = 51)\n",
    "#             augmented_images.append(flip_s_p_img)\n",
    "#             image_labels.append(label)\n",
    "        \n",
    "        #4 rotated image: loop 0 to 2 to save all rotation ops\n",
    "        for rot_angle in rot_angles:\n",
    "            # rotate image randomly: 15 to 270 degrees counterclockwise\n",
    "            M = cv2.getRotationMatrix2D(center, rot_angle, scale)\n",
    "            rotated_image = cv2.warpAffine(image, M, (w, h))\n",
    "            augmented_images.append(rotated_image)\n",
    "            image_labels.append(label)                     \n",
    "            \n",
    "            #4.1 Add Salt and Pepper Noise to rotated image\n",
    "#             rot_s_p_img = add_salt_pepper_noise(rotated_image, pad = 101)\n",
    "#             augmented_images.append(rot_s_p_img)\n",
    "#             image_labels.append(label)            \n",
    "            \n",
    "        #5 translate image: loop 0 to 3 to save all translations\n",
    "#         for dir_op in range(0, 4):\n",
    "#             # Shift object in the horizontal and vertical direction by 8 pixels\n",
    "#             # retaining 75% percent of the base image\n",
    "#             # [4,4] shift down and to the right by 8 pixels\n",
    "#             # [-4, 4] shift up by -8 pixels and to the right 8 pixels\n",
    "#             # [-4, -4] shift up by -8 pixels and to the left by -8 pixels\n",
    "#             # [4, -4] shift down by 8 pixels and to the left by -8 pixels\n",
    "#             quarter_height, quarter_width = h / shift[dir_op][0], w / shift[dir_op][1]\n",
    "\n",
    "#             # create transformation matrix where [1,0,tx], tx denotes shift along x-axis,\n",
    "#             # [0,1,ty], ty denotes shift along y-axis\n",
    "#             T = np.float32([[1, 0, quarter_width], [0, 1, quarter_height]])\n",
    "\n",
    "#             # We use warpAffine to transform \n",
    "#             # the image using the matrix, T \n",
    "#             translated_image = cv2.warpAffine(image, T, (w, h)) \n",
    "#             augmented_images.append(translated_image)\n",
    "#             image_labels.append(label)              \n",
    "            \n",
    "            #5.1 Add Salt and Pepper Noise to translated image\n",
    "#             tran_s_p_img = add_salt_pepper_noise(translated_image, pad = 51)\n",
    "#             augmented_images.append(tran_s_p_img)\n",
    "#             image_labels.append(label)             \n",
    "            \n",
    "    return augmented_images, image_labels\n",
    "\n",
    "    \n",
    "# Replace existing training dataset with\n",
    "augmented_data, augmented_labels = [], []\n",
    "augmented_data, augmented_labels = augment_data(X_train, y_train)\n",
    "X_train = augmented_data\n",
    "y_train = augmented_labels\n",
    "print(X_train[0].shape)\n",
    "# data augmentation applied using OpenCV\n",
    "# X_train = cv_flip_images(X_train_original)\n",
    "print(type(X_train))\n",
    "print(len(X_train))\n",
    "print(type(y_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the training data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TensorFlow\n",
    "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implement LeNet-5\n",
    "Implement the [LeNet-5](http://yann.lecun.com/exdb/lenet/) neural network architecture.\n",
    "\n",
    "This is the only cell you need to edit.\n",
    "### Input\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "### Architecture\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using `tf.contrib.layers.flatten`, which is already imported for you.\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 5: Fully Connected (Logits).** This should have 10 outputs.\n",
    "\n",
    "### Output\n",
    "Return the result of the 2nd fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x, keep_prob):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # TODO: Layer 1: Convolutional. Input = 32x32x3. Output = 28x28x64.\n",
    "    # The formula for convolutions tell us:\n",
    "    # out_h = (in_h - filter_h + 1)/strides[1]\n",
    "    # out_w = (in_w - filter_w + 1)/strides[2]\n",
    "    # this layer has a 3x3 filter with in_depth 3 and out_depth 12\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5,5,3,64), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(64))\n",
    "    # we use conv2d to convolve the filter over the imgs and add bias at end\n",
    "    conv1 = tf.nn.conv2d(x, conv1_W, strides = [1,1,1,1], padding = 'VALID') + conv1_b\n",
    "\n",
    "    # TODO: Activation.\n",
    "    # we activate the conv1 of the convolutional layer with relu activation function\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    # TODO: Pooling. Input = 28x28x64. Output = 14x14x64.\n",
    "    # we pool the conv1 using 2x2 kernel with 2x2 stride\n",
    "    # filter size = 2x2\n",
    "    # moves at stride of 2\n",
    "    # out_h = (in_h - filter_h)/S + 1\n",
    "    # out_w = (in_w - filter_w)/S + 1\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    # the network then runs through another set of convolutional, activation and pooling layers outputting 5x5x16\n",
    "    \n",
    "    # TODO: Layer 2: Convolutional. Output = 10x10x32.\n",
    "    # use same hxw dimension as conv1\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5,5,16,32), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(32))\n",
    "    # move at stride of 1\n",
    "    conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1,1,1,1], padding = 'VALID') + conv2_b\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    # TODO: Pooling. Input = 10x10x16. Output = 5x5x32.\n",
    "    # filter size = 2\n",
    "    # moves at stride of 2\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')  \n",
    "    \n",
    "    # TODO: Flatten. Input = 5x5x32. Output = 800.\n",
    "    # we flatten this output into a vector, the length of the vector =  5x5x16 = 400.\n",
    "    fc0 = flatten(conv2)\n",
    "    \n",
    "    # TODO: Layer 3: Fully Connected. Input = 800. Output = 120.\n",
    "    # we pass this vector into a fully connected layer with a width of 120\n",
    "    # shape=(height, width)\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(800,120), mean = mu, stddev = sigma))\n",
    "    # tf.zeros(120) is 1D array with 120 elements that are 0\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    # x*W + b\n",
    "    fc1 = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    # then we apply relu to the output of the fc1\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    # Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "    \n",
    "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    # we repeat the fc again with width of 84\n",
    "    fc2_W = tf.Variable(tf.truncated_normal(shape=(120,84), mean = mu, stddev = sigma))\n",
    "    fc2_b = tf.Variable(tf.zeros(84))\n",
    "    # x*W + b\n",
    "    fc2 = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    # Dropout\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "    \n",
    "    # TODO: Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    # finally we attach a fc output layer with a width equal to the number of classes in our label set\n",
    "    # in this case, we have 43 classes, one for traffic sign\n",
    "    fc3_W = tf.Variable(tf.truncated_normal(shape=(84,43), mean = mu, stddev = sigma))\n",
    "    fc3_b = tf.Variable(tf.zeros(43))\n",
    "    # x*W + b\n",
    "    # these outputs are also known as our logits, which is what we return from our lenet function\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels\n",
    "Train LeNet to classify [MNIST](http://yann.lecun.com/exdb/mnist/) data.\n",
    "\n",
    "`x` is a placeholder for a batch of input images.\n",
    "`y` is a placeholder for a batch of output labels.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the batch of input imgs to none, which will allow the placeholder to later accept a batch of any size\n",
    "# img dimensions are set to 32x32x1\n",
    "# 1 because grayscale, if it was rgb, then 3\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "# y stores our labels, our labels come through as sparse variables, they are ints they aren't one hot encoded yet\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "# we use tf.one_hot function to one hot encode the labels\n",
    "one_hot_y = tf.one_hot(y, 43)\n",
    "# probability to keep units\n",
    "keep_prob = tf.placeholder(tf.float32) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "Create a training pipeline that uses the model to classify MNIST data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\james\\Anaconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000022C9A323C88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000022C9A323C88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000022C9A323C88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000022C9A323C88>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From <ipython-input-7-97561653dd90>:65: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-9-737545565728>:10: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is our training pipeline, you have to pass data into it for it to work\n",
    "\n",
    "# we have another hyperparameter, learning rate tells tensorflow how quickly to update the networks weights\n",
    "rate = 0.001\n",
    "# we pass the input data to LeNet function to calculate our logits\n",
    "logits = LeNet(x, keep_prob)\n",
    "# we use the softmax cross entropy function to compare those logits to the ground truth labels\n",
    "# and calculate the cross entropy. Cross entropy is a measure of how different the logits are\n",
    "# from the ground truth labels\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "# The tf.reduce_mean function averages the cross entropy from all the training imgs\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "# Adam optimizer uses the Adam algorithm to minimize the loss function\n",
    "# Similar to the stochastic gradient descent, Adam is a little more sophisticated than it\n",
    "# this is where we use the learning rate hyperparameter set earlier\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "# finally we use minimize function on the optimizer, which uses backpropagation to update the network\n",
    "# and minimize our training loss\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate how well the loss and accuracy of the model for a given dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate pipeline evaluates how good the model is\n",
    "\n",
    "# the below two functions are the entire evaluation pipeline, but in order to run this\n",
    "# evaluation pipeline, we have to build an evaluate function\n",
    "\n",
    "# the first step in this pipeline is to measure whether a given prediction is correct\n",
    "# by comparing the logit prediction to the one hot encoded ground truth label\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "\n",
    "# the second step is to calculate the model's overall accuracy by averaging the individual\n",
    "# prediction accuracies\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# this function takes a dataset as input, sets some initial variables\n",
    "# then batches the dataset and runs it through the evaluation pipeline\n",
    "# the evaluate function averages the accuracy of each batch to calculate the total\n",
    "# accuracy of the model\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Run the training data through the training pipeline to train the model.\n",
    "\n",
    "Before each epoch, shuffle the training set.\n",
    "\n",
    "After each epoch, measure the loss and accuracy of the validation set.\n",
    "\n",
    "Save the model after training.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Train Accuracy = 0.969\n",
      "Validation Accuracy = 0.936\n",
      "\n",
      "EPOCH 2 ...\n",
      "Train Accuracy = 0.984\n",
      "Validation Accuracy = 0.951\n",
      "\n",
      "EPOCH 3 ...\n",
      "Train Accuracy = 0.990\n",
      "Validation Accuracy = 0.965\n",
      "\n",
      "EPOCH 4 ...\n",
      "Train Accuracy = 0.995\n",
      "Validation Accuracy = 0.966\n",
      "\n",
      "EPOCH 5 ...\n",
      "Train Accuracy = 0.994\n",
      "Validation Accuracy = 0.969\n",
      "\n",
      "EPOCH 6 ...\n",
      "Train Accuracy = 0.996\n",
      "Validation Accuracy = 0.961\n",
      "\n",
      "EPOCH 7 ...\n",
      "Train Accuracy = 0.997\n",
      "Validation Accuracy = 0.971\n",
      "\n",
      "EPOCH 8 ...\n",
      "Train Accuracy = 0.997\n",
      "Validation Accuracy = 0.973\n",
      "\n",
      "EPOCH 9 ...\n",
      "Train Accuracy = 0.995\n",
      "Validation Accuracy = 0.964\n",
      "\n",
      "EPOCH 10 ...\n",
      "Train Accuracy = 0.997\n",
      "Validation Accuracy = 0.976\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# now everything is setup, we can build a function to train and evaluate our model\n",
    "# first we create the tensorflow session and initialize the variables\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    # we train over whatever number has been set in the EPOCHS hyperparameter\n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        # at the beginning of each EPOCH, we shuffle our training data to ensure that our training isn't bias\n",
    "        # by the order of the imgs\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        # then we break our training data into batches and train the model on each batch\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            # keep_prob = 0.5, accuracy = 94%\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 0.5})\n",
    "            \n",
    "        # at the end of each EPOCH, we evaluate the model on our training data\n",
    "        train_accuracy = evaluate(X_train, y_train)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Train Accuracy = {:.3f}\".format(train_accuracy))           \n",
    "            \n",
    "        # at the end of each EPOCH, we evaluate the model on our validation data\n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    # once we have completely trained the model we save it that way we can load it up later and modify it\n",
    "    # or evaluate the model on our test dataset\n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "Once you are completely satisfied with your model, evaluate the performance of the model on the test set.\n",
    "\n",
    "Be sure to only do this once!\n",
    "\n",
    "If you were to measure the performance of your trained model on the test set, then improve your model, and then measure the performance of your model on the test set again, that would invalidate your test results. You wouldn't get a true measure of how well your model would perform against real data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\james\\Anaconda3\\envs\\IntroToTensorFlow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from .\\lenet\n",
      "Test Accuracy = 0.953\n"
     ]
    }
   ],
   "source": [
    "# we evaluate the model on our test dataset\n",
    "# we should only run the model on the test dataset one time once we are completely done with training\n",
    "# otherwise we would use the test dataset to choose the best model and then the test dataset wouldn't\n",
    "# provide a good estimate of how well the model would do in the real world\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
