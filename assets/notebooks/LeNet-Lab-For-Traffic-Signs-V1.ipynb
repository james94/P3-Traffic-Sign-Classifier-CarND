{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet Lab\n",
    "![LeNet Architecture](assets/images/lenet.png)\n",
    "Source: Yan LeCun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the **German Traffic Sign Dataset**, which comes pre-loaded with TensorFlow.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = 'assets/data/train.p'\n",
    "validation_file='assets/data/valid.p'\n",
    "testing_file= 'assets/data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(validation_file, mode='rb') as f:\n",
    "    valid = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_valid, y_valid = valid['features'], valid['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data\n",
    "\n",
    "View a sample from the dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFgAAABYCAYAAABxlTA0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAPtklEQVR4nO2cW4hlV1rHf9/e55x9LnXt6uqu7ly6kzEmg0YSkAl4gUEjiD7M+KDMPAyjDMQHBxR8cPBBxKcR1Fch4oAPgggKBhmQQRxBGEI0RGd6ktiZpNPX6u50V1VXnfve6/Ph+9Y+p7urqi/V51RMzgfF3nX2Wmuv/e3/+u5ri6oyo8lRctgT+KTTjMETphmDJ0wzBk+YZgyeMM0YPGE6EINF5JdF5F0ReU9EvvGoJvVJInlYO1hEUuB/gV8CLgJvAF9W1R8+uun9/6fKAfp+DnhPVd8HEJG/A74A7MngZrOpS0tLiAiJCN7PLiYpAIPugPb2JgCFhrvGKNs73QaQeOk2zOwNoDuvqCrBxxPi/HZpKLdfG+Y5eVEIu9BBGPwYcGHs/4vAS3c2EpFXgFcAFhcWeeVrr1DNajQyu3WlYlIqzRYB+OCHl3j9318DYKt3K45Cmtj809T6pWL9+oMewR+tktpvqgGN3I4vKR4lITjDQjzBruXDnM5waM38PrUkhTtWeeL3qaYpQQMfXLyyJ5MOwuDd3thdcFHVV4FXAV54/nn9nd/6CokqJMGv2wMVXev6Bm/xo+8Zmjt9u0WBjJjhWiMdO4bCxiocWUmSlA+m3qEYm2AIcWV4e39ZlUqFWmSmpD6+lAzO/T7DYvSYuwF8nA7C4IvAE2P/Pw5c3q/DcNjn6qUPKPIB7ZADkFfsQead4a35q2RVY3p88ICgRCQaq6J2TpOEoTMshMjGJK7icskXYSRuopiJCEnKE6j6mys0Ml/GxFC8j/h/STnHveggVsQbwDMi8pSI1IAvAa8dYLxPJD00glU1F5GvA/8CpMC3VPXMfn0uX17nj/7kTwkhp9+sAvDcSy8D8Ks//3MA9GWRtNaye9AFDH0ityufKDJ2M4KCBoLeqQzHFdNd6i02Ks9DbqshF0GSiGZbbdW4PFRB9rfCDiIiUNVvA98+yBifdDoQgx+UOv0B/332QyopVOcaAHz2J+3a06tPAfDR5gZZLQMMiWAWg/r5SN4acpJESJIo6UZoGukxR24ykrtxXI1o1VHvkWXhii1ANbHVVknHkAsUqr6y9kbxzFWeME0VwapKP+QEErRvMq7mcnapYaiShRrzbiMnjowCSNwRiRo9dxQ2Kg0WM8PJMPQBaHeHRFSljm7x/zUoJLfjaoR/pVpxpOvA76OlBVMEH0tG7TWEfe20qTIYABWKEEjdphz03bD3WS/MtVhompKLzsQwBHsQxvSTM7xVb3K06Q5GWgPgum5zqzvwMW63dQNjDFG9/Ugo7evgJ0lelC81ipSoPkMI7tTMRMSh0fQRjCmVaPj3B4X/5i5zrUaa1b3lyNiPDkO0vtKKKZ7FRsZKMDEjwVZDtTVPPzc3exjuUGQ6vqLHTDfMlIu6UaNIkjBSouUYOpoXSWk67kYzBE+Ypo5gDRYriGKv3zNZOczth2papVY3WRpNs6DJWATLXetm047pkNr2NgCJu7BL9SbHGybHr7QNya5TEZSiNMWk/A3MlIuyOimDQzLmWsc5RDMtsHtIZkTTZ7AqOraohv7kUVS0koRG3ezguL40D6VoqHiUa8nFSJa3qfljiPix6LBasTE62QIA111kFEUorYIRm2PgQkZWR4wtJZXypcaYhXg8JFcdyaw9aCYiJkxTR7CIkoiSOmrUff7Cj0kzJXUESwmZUCq5tGbK7WjNpt4aFDSPHAegPrcMwPVrZ5lzfK7VDen5sAfAZr9PP7/dkyuVXNCRnV1EGzwp7504cmPULqiCJPuZwTMET5oOBcGgZTyg3zdk9Xt2DE0hzWre1lGOlpmP+YbJw0bRAaCW1lg49QwAyydP2Vhne3z04YfWPjUEr7Ys9tEeDhj4uKXpNwbBwh0gX1BoIqVHGWW3uJyuSgLIXWmscZoheMJ0KFZECIHC1XSna8jtDcxcC0mFSmZyVpJRDLiamlxerNq1pLB+9eXjNB4z5CarJwFYKl6g37W4xOZlSxvO1+YBWG5lDHYsmxJyXyFhBOGYy9MydlGMzDR3Pso8LRYj2c+OmHqwJwRTcpLaA3R7HqApGVwr7eCY6KykKQuurJajV+UhxNaJU1SXTbnhQaL51SfonbwKQH/LMtS6Y97eWr1Ox03c4c4OYGFHsBBnyeAxRRai6eY2so6xVO5R9jATEROmKYsIAUYpGID+0BC83bdjUVkma5pCSlNfkqnSzOy8WRjSs3lb8vOrj5U1Futb1wFo1RssPflZAIY7Nm7/vbes3xDWMusrhYmKGx0TN+PaLvoPoQhojMSVdRoRl2LX93niGYInTNNXcqPQNwADdwBuDTzBKVB1JVepGmrrIaVVsfYNjzfMLZ+w9lXhnbNnAejUDfmnT52ivnAEgLVnf8Ku3TKZvHXxEkccckW2BEAv37I2/d4Ixb7KtFIpzbKYto+x6SJJ2cdCs2e4F0MeKYl5ZyKC+sxyLMTYy43BQQOZe2nVmim7ZlKnpfYiqr68m8eeBOBiZ8A710w0rC2ZsmuGlKTij9ayWMSJHzNG97a36G9aXGIls/b93CyUa/mAQe6FMGUByqi0JAaC8hC9z/yOuom7aSYiJkxTRXClWuXoyTUSEupiKFhwz6ySDL1VIPN4Q5bZkp8LKU3Ph2VLx6zfmmWh07TCtXVb/hs3DcnXN7ZZW7Fat6pH3ZrHTwOw9uxNLp95EwAZGJJPzFmbblFlo+3zkJjLYyQ2JNbHxbySluJiL5oheMI0VQTXshpPnj7FcDCAHXMAal6W1+u1ATP2s5oh98iCBdVb+Q6tYEhfOGnKrbVkAfUGNZ5cMLn8z++aKba+ucGTKyZfl5dNBh9dNqU3d+QpVk/bPdfffQOArDA2HF9YoJubWdfJzRxEx1JCJZBHSFbZP+R+TwSLyBMi8m8i8raInBGR3/Xfj4jId0TkrB+X7zXWp5HuB8E58Puq+qaIzAP/JSLfAX4T+FdV/aZvH/gG8Af7DTTo9jj3zrseTzXZVWkZSs9vGKK7hdLwaNpq3VAbtM/iwhoAzZVVAJKaYWNrY5N239CWOMqvXjjPjQvnbXw37+ZX5gB48cWXObX24/bbTSsG3b6yDsBiNWFlzky3zd4NAIo8lPKYMgoXq4sCsL+jcU8Gq+oV4Iqfb4vI21jx9ReAz3uzvwG+yz0YrKr0uz0SSUozrejZ9NpdL2fNczK180WMcVtpQsOVWmP1cXvWqtf0ZoHMTbGfeekXAEgqkA88xrFlxdHnt84B8O76Osc+8zwAK6d/CoB+22IS3a02y6mNdXLe5nClvU3w4PvIGo4mZti1+HCcHkgGi8hp4EXgdeC4Mx9VvSIix/boU1a4VyqHUiVwqHTfTywic8A/AL+nqrf2CzKP03iFe5bVNYSCQCgjZXhsISq5zvYmNy+cs2u3DFnV1gJzTzhyG55x9lDm4tIyTz/r4c3Ujo16jVinl3tW+cy6hS0/2Oxz+ZqdP/+4KcxjT1vAfv3M9ym6lqFeq5no6ubKdtfG0LJkNqaQTO8dSMkBiEgVY+7fquo/+s9XReSEXz8BXLufsT5tdE8Ei0H1r4G3VfUvxi69BnwV+KYf/+leY4UQ2Gl3QIRq1W6dVQ2R2y4Ht29e59LZ9wDIO2b019eOUls02Rg3oMQ4rAjMLZoCiyn3SiLlVoNUDInPPXEagBPLO2zdMIW6uWOrZmHlKACLx1bpnTN0LyXmfJzMqlwY2srYyWPSM9bJ6T23ENyPiPhZ4CvA90XkLf/tDzHG/r2IfA04D/z6fYz1qaP7sSL+g73FzC8+yM1UzewRSUZZAsy0am9tAHD+7BkuvW8Jy4AFYVaPn6ZSM0SJb55JfUzRnlVJA1XX9hKG4EGb0DaUZj1Lks732uQdszBuXLoJQIJnNra2SWOwJ9h8FhvzbHmVUL9rAak49jAU3MNTnna40qvBVct9a+pKbvOi2a0fygbbvoTr9RUAkmLIzlULSebBomrF0MObgz5Fx70uL1kddrqop6AKZ3DuAf3BcEjfxdLQ+7Xde8uDErwENk+MNYGkFGdJ9/adTpU0Lesn9qJZLGLCdAgBd9vXUO4S8uXd8YjY5eENGgMTA8NgS/fa229BYihTr2KPazPkRbnLMA0jj6vwctg81lbEPRr1RpmKSjxAnzvOhoXS9zRSb2iDdgYDtrxmI14rypi8kqayb9B9huAJ09STnpb6Hr3X1KfQcIXWbfeouhy0MAgMBptoLOmX6FRYm6Salr+VclMSeoVvLteY3nH5LOqjgnrBdttT+je22/R6dp4Po7JTBp4BzUuNNtplBLKvu3wIhSexfCoW4Pmy8/8fO/0Mq66EbmxYwOWjzi3avpndK5tG21k1LW2cGFas1FpUW6b5xYP3XkZBNctouJtXxZa+rluw53LnHNtROUYrRCECotyqW+btrHRqP5qJiAnT1JOeSZqSAOIo6rtd2/aSqM99/mWO+Wv/7huvA3Dmex+y5QXaqVvArSUz4R4/fYqjR80TW5o3j67VyMi8ULuI2eG4lTZAf9NWxo8uWIopioBmpUK3jK3HTeGB4GbZXbXWwbZ4zeoiDpGmimBBSNKUikBSbkt12eqKqnX8MVbmTOHpD2xveS9PGAz8EweOzFbV0kSV1lHU6xtwhwDtUuQeX/aCvTx3B6LXp/CxSM1Mq3vitVlvcqNErvW3Oo7RriIYK6uNq2KfZ54heMI0VQQnidDI6rQqaRmR6sfPu7hm7pNSeLq+1TKZWk0SJJaYiu8Q9UqgXrdbfvFkuGnIrOgWVa8K2nYvejh0BPdvkXjJQHPeInSp2231VrOEXJSs40UlEblJrMbcd4+n0XTrIipVjq0c57mVZW61Ld7w/k07Vn0nT6FaVvQsue3bADbiJkB/MTEl1EiF1ZYFguYzEy294ZAdz891h/5CnBX1+TnEv66SeT/xNu0so+JfYCnLUlXHRMLt7BRRD8LPKnsOjR76u2kPdTOR60Ab+GhqN314Osr9z/OUqq7udmGqDAYQkf9U1Z+e6k0fgh7VPGciYsI0Y/CE6TAY/Ooh3PNh6JHMc+oy+NNGMxExYZoxeMI0NQZ/nD/mvE+J7h+LyCURecv/fuWBx56GDP64f8zZS79OjJfoAl8EfgPYUdU/e9ixp4Xg8mPOqjoA4secPxakqldU9U0/3wZiie6BaVoM3u1jzo/kAR413VGiC/B1EfkfEfnWw1TxT4vB9/Ux58OmO0t0gb8EPgO8gBWh//mDjjktBj/wx5ynTbuV6KrqVVUt1D5/9VeYqHsgmhaDP9Yfc96rRDfWPzv9GvCDBx17KgH3h/mY85RprxLdL4vIC5g4Owf89oMOPHOVJ0wzT27CNGPwhGnG4AnTjMETphmDJ0wzBk+YZgyeMP0fsdK1hHqq1n0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = random.randint(0, len(X_train))\n",
    "image = X_train[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image)\n",
    "print(y_train[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Data\n",
    "\n",
    "Shuffle the training data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup TensorFlow\n",
    "The `EPOCH` and `BATCH_SIZE` values affect the training speed and model accuracy.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implement LeNet-5\n",
    "Implement the [LeNet-5](http://yann.lecun.com/exdb/lenet/) neural network architecture.\n",
    "\n",
    "This is the only cell you need to edit.\n",
    "### Input\n",
    "The LeNet architecture accepts a 32x32xC image as input, where C is the number of color channels. Since MNIST images are grayscale, C is 1 in this case.\n",
    "\n",
    "### Architecture\n",
    "**Layer 1: Convolutional.** The output shape should be 28x28x6.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 14x14x6.\n",
    "\n",
    "**Layer 2: Convolutional.** The output shape should be 10x10x16.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Pooling.** The output shape should be 5x5x16.\n",
    "\n",
    "**Flatten.** Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using `tf.contrib.layers.flatten`, which is already imported for you.\n",
    "\n",
    "**Layer 3: Fully Connected.** This should have 120 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 4: Fully Connected.** This should have 84 outputs.\n",
    "\n",
    "**Activation.** Your choice of activation function.\n",
    "\n",
    "**Layer 5: Fully Connected (Logits).** This should have 10 outputs.\n",
    "\n",
    "### Output\n",
    "Return the result of the 2nd fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1\n",
    "    \n",
    "    # TODO: Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x12.\n",
    "    # The formula for convolutions tell us:\n",
    "    # out_h = (in_h - filter_h + 1)/strides[1]\n",
    "    # out_w = (in_w - filter_w + 1)/strides[2]\n",
    "    # this layer has a 3x3 filter with in_depth 3 and out_depth 12\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(shape=(5,5,3,6), mean = mu, stddev = sigma))\n",
    "    conv1_b = tf.Variable(tf.zeros(6))\n",
    "    # we use conv2d to convolve the filter over the imgs and add bias at end\n",
    "    conv1 = tf.nn.conv2d(x, conv1_W, strides = [1,1,1,1], padding = 'VALID') + conv1_b\n",
    "\n",
    "    # TODO: Activation.\n",
    "    # we activate the conv1 of the convolutional layer with relu activation function\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    # TODO: Pooling. Input = 28x28x12. Output = 14x14x12.\n",
    "    # we pool the conv1 using 2x2 kernel with 2x2 stride\n",
    "    # filter size = 2x2\n",
    "    # moves at stride of 2\n",
    "    # out_h = (in_h - filter_h)/S + 1\n",
    "    # out_w = (in_w - filter_w)/S + 1\n",
    "    conv1 = tf.nn.max_pool(conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    # the network then runs through another set of convolutional, activation and pooling layers outputting 5x5x16\n",
    "    \n",
    "    # TODO: Layer 2: Convolutional. Output = 10x10x24.\n",
    "    # use same hxw dimension as conv1\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(shape=(5,5,6,16), mean = mu, stddev = sigma))\n",
    "    conv2_b = tf.Variable(tf.zeros(16))\n",
    "    # move at stride of 1\n",
    "    conv2 = tf.nn.conv2d(conv1, conv2_W, strides=[1,1,1,1], padding = 'VALID') + conv2_b\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    # TODO: Pooling. Input = 10x10x24. Output = 5x5x24.\n",
    "    # filter size = 2\n",
    "    # moves at stride of 2\n",
    "    conv2 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "    \n",
    "    # TODO: Flatten. Input = 5x5x16. Output = 400.\n",
    "    # we flatten this output into a vector, the length of the vector =  5x5x30 = 750.\n",
    "    fc0 = flatten(conv2)\n",
    "    \n",
    "    # TODO: Layer 3: Fully Connected. Input = 750. Output = 120.\n",
    "    # we pass this vector into a fully connected layer with a width of 120\n",
    "    # shape=(height, width)\n",
    "    fc1_W = tf.Variable(tf.truncated_normal(shape=(400,120), mean = mu, stddev = sigma))\n",
    "    # tf.zeros(120) is 1D array with 120 elements that are 0\n",
    "    fc1_b = tf.Variable(tf.zeros(120))\n",
    "    # x*W + b\n",
    "    fc1 = tf.matmul(fc0, fc1_W) + fc1_b\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    # then we apply relu to the output of the fc1\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "    # TODO: Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    # we repeat the fc again with width of 84\n",
    "    fc2_W = tf.Variable(tf.truncated_normal(shape=(120,84), mean = mu, stddev = sigma))\n",
    "    fc2_b = tf.Variable(tf.zeros(84))\n",
    "    # x*W + b\n",
    "    fc2 = tf.matmul(fc1, fc2_W) + fc2_b\n",
    "    \n",
    "    # TODO: Activation.\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    \n",
    "    # TODO: Layer 5: Fully Connected. Input = 84. Output = 43.\n",
    "    # finally we attach a fc output layer with a width equal to the number of classes in our label set\n",
    "    # in this case, we have 43 classes, one for traffic sign\n",
    "    fc3_W = tf.Variable(tf.truncated_normal(shape=(84,43), mean = mu, stddev = sigma))\n",
    "    fc3_b = tf.Variable(tf.zeros(43))\n",
    "    # x*W + b\n",
    "    # these outputs are also known as our logits, which is what we return from our lenet function\n",
    "    logits = tf.matmul(fc2, fc3_W) + fc3_b\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features and Labels\n",
    "Train LeNet to classify [MNIST](http://yann.lecun.com/exdb/mnist/) data.\n",
    "\n",
    "`x` is a placeholder for a batch of input images.\n",
    "`y` is a placeholder for a batch of output labels.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the batch of input imgs to none, which will allow the placeholder to later accept a batch of any size\n",
    "# img dimensions are set to 32x32x1\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "# y stores our labels, our labels come through as sparse variables, they are ints they aren't one hot encoded yet\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "# we use tf.one_hot function to one hot encode the labels\n",
    "one_hot_y = tf.one_hot(y, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "Create a training pipeline that uses the model to classify MNIST data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\james\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\layers\\python\\layers\\layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000026CC3F2DF98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000026CC3F2DF98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000026CC3F2DF98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x0000026CC3F2DF98>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:From <ipython-input-7-ff1e765c4cee>:10: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this is our training pipeline, you have to pass data into it for it to work\n",
    "\n",
    "# we have another hyperparameter, learning rate tells tensorflow how quickly to update the networks weights\n",
    "rate = 0.001\n",
    "# we pass the input data to LeNet function to calculate our logits\n",
    "logits = LeNet(x)\n",
    "# we use the softmax cross entropy function to compare those logits to the ground truth labels\n",
    "# and calculate the cross entropy. Cross entropy is a measure of how different the logits are\n",
    "# from the ground truth labels\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=one_hot_y, logits=logits)\n",
    "# The tf.reduce_mean function averages the cross entropy from all the training imgs\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "# Adam optimizer uses the Adam algorithm to minimize the loss function\n",
    "# Similar to the stochastic gradient descent, Adam is a little more sophisticated than it\n",
    "# this is where we use the learning rate hyperparameter set earlier\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "# finally we use minimize function on the optimizer, which uses backpropagation to update the network\n",
    "# and minimize our training loss\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate how well the loss and accuracy of the model for a given dataset.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate pipeline evaluates how good the model is\n",
    "\n",
    "# the below two functions are the entire evaluation pipeline, but in order to run this\n",
    "# evaluation pipeline, we have to build an evaluate function\n",
    "\n",
    "# the first step in this pipeline is to measure whether a given prediction is correct\n",
    "# by comparing the logit prediction to the one hot encoded ground truth label\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "\n",
    "# the second step is to calculate the model's overall accuracy by averaging the individual\n",
    "# prediction accuracies\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# this function takes a dataset as input, sets some initial variables\n",
    "# then batches the dataset and runs it through the evaluation pipeline\n",
    "# the evaluate function averages the accuracy of each batch to calculate the total\n",
    "# accuracy of the model\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "Run the training data through the training pipeline to train the model.\n",
    "\n",
    "Before each epoch, shuffle the training set.\n",
    "\n",
    "After each epoch, measure the loss and accuracy of the validation set.\n",
    "\n",
    "Save the model after training.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Validation Accuracy = 0.688\n",
      "\n",
      "EPOCH 2 ...\n",
      "Validation Accuracy = 0.800\n",
      "\n",
      "EPOCH 3 ...\n",
      "Validation Accuracy = 0.838\n",
      "\n",
      "EPOCH 4 ...\n",
      "Validation Accuracy = 0.856\n",
      "\n",
      "EPOCH 5 ...\n",
      "Validation Accuracy = 0.869\n",
      "\n",
      "EPOCH 6 ...\n",
      "Validation Accuracy = 0.877\n",
      "\n",
      "EPOCH 7 ...\n",
      "Validation Accuracy = 0.876\n",
      "\n",
      "EPOCH 8 ...\n",
      "Validation Accuracy = 0.881\n",
      "\n",
      "EPOCH 9 ...\n",
      "Validation Accuracy = 0.895\n",
      "\n",
      "EPOCH 10 ...\n",
      "Validation Accuracy = 0.895\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# now everything is setup, we can build a function to train and evaluate our model\n",
    "# first we create the tensorflow session and initialize the variables\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    # we train over whatever number has been set in the EPOCHS hyperparameter\n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    for i in range(EPOCHS):\n",
    "        # at the beginning of each EPOCH, we shuffle our training data to ensure that our training isn't bias\n",
    "        # by the order of the imgs\n",
    "        X_train, y_train = shuffle(X_train, y_train)\n",
    "        # then we break our training data into batches and train the model on each batch\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_train[offset:end], y_train[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        # at the end of each EPOCH, we evaluate the model on our validation data\n",
    "        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "        print()\n",
    "        \n",
    "    # once we have completely trained the model we save it that way we can load it up later and modify it\n",
    "    # or evaluate the model on our test dataset\n",
    "    saver.save(sess, './lenet')\n",
    "    print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "Once you are completely satisfied with your model, evaluate the performance of the model on the test set.\n",
    "\n",
    "Be sure to only do this once!\n",
    "\n",
    "If you were to measure the performance of your trained model on the test set, then improve your model, and then measure the performance of your model on the test set again, that would invalidate your test results. You wouldn't get a true measure of how well your model would perform against real data.\n",
    "\n",
    "You do not need to modify this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we evaluate the model on our test dataset\n",
    "# we should only run the model on the test dataset one time once we are completely done with training\n",
    "# otherwise we would use the test dataset to choose the best model and then the test dataset wouldn't\n",
    "# provide a good estimate of how well the model would do in the real world\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy = {:.3f}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
